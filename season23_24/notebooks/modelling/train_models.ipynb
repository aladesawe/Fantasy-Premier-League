{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# which game in the future are you trying to predict? shift_param=1 means the next game (2 means the one after that etc.)\n",
    "shift_param = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import catboost \n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(y_true, y_predicted):\n",
    "    mae = mean_absolute_error(y_true, y_predicted)\n",
    "    rmse = mean_squared_error(y_true, y_predicted, squared=False)\n",
    "    r2 = r2_score(y_true, y_predicted)\n",
    "    return (mae, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE OPTUNA OBJECTIVE FOR HYPER-PARAMETER OPTIMIZATION\n",
    "def optuna_objective(trial, model_name, optuna_params, X_train, y_train, folds, test_metric):\n",
    "    '''\n",
    "    Objective function for Optuna to optimize.\n",
    "\n",
    "    Inputs:\n",
    "        trial: Optuna trial object.\n",
    "        model_name (str): String to specify which model type to optimize. Current options: {catboost}.\n",
    "        optuna_params (dict): Optuna-specific parameters. \n",
    "        X_train (array-like): Training data inputs.\n",
    "        Y_train (array-like): Training data target values.\n",
    "        folds (sklearn KFold): Cross-validation folds.\n",
    "        test_metric (str): Chosen metric for evaluating model performance in the test set.\n",
    "\n",
    "    Output:\n",
    "        best_result (float): Mean of the test-metric across cross-validation folds.\n",
    "    '''\n",
    "\n",
    "    if model_name=='catboost':   \n",
    "        params = {}\n",
    "        params['learning_rate'] = trial.suggest_float('learning_rate', \n",
    "                                                    optuna_params['cat_learning_rate_bounds'][0],\n",
    "                                                    optuna_params['cat_learning_rate_bounds'][1],\n",
    "                                                    log=True)\n",
    "        params['depth'] = trial.suggest_int('depth', optuna_params['cat_depth_bounds'][0], \n",
    "                                                optuna_params['cat_depth_bounds'][1])\n",
    "        params['objective'] = trial.suggest_categorical('objective', optuna_params['cat_objective_list'])\n",
    "        params['eval_metric'] = test_metric\n",
    "        \n",
    "        cv_data = catboost.Pool(\n",
    "            data=X_train,\n",
    "            label=y_train,\n",
    "        )\n",
    "\n",
    "        cv_df = catboost.cv(\n",
    "            pool=cv_data,\n",
    "            params=params,\n",
    "            folds=folds,\n",
    "            num_boost_round=1000, \n",
    "            early_stopping_rounds=50, \n",
    "            #verbose_eval=False,\n",
    "            logging_level='Silent',\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "        best_result = cv_df.sort_values(f'test-{test_metric}-mean')[f'test-{test_metric}-mean'].iloc[0]        \n",
    "\n",
    "    else:\n",
    "        print('Incorrect model name!')\n",
    "        return\n",
    "    \n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_optuna(X_train, y_train, X_test, y_test, cat_optuna_params, \n",
    "               best_params, metrics, folds, test_metric, n_optimization_trials=100):\n",
    "    '''\n",
    "    Performs hyperparameter optimization for the CatBoost regressor using Optuna.\n",
    "\n",
    "    Input:\n",
    "        X_train (array-like): The training input samples.\n",
    "        Y_train (array-like): The target values for the training set.\n",
    "        X_test (array-like): The test input samples.\n",
    "        Y_test (array-like): The target values for the test set.\n",
    "        cat_optuna_params (dict): Parameters for Optuna optimization.\n",
    "        best_params (dict): A dictionary to store the best hyperparameters found during optimization.\n",
    "        metrics (dict): A dictionary for saving model performance results.\n",
    "        model_spec (dict): A dictionary defining inputs, the target and the training and test periods.\n",
    "        n_optimization_trials (int): Number of optimization trials to be performed with Optuna.\n",
    "        \n",
    "    Output:\n",
    "        metrics (dict): A dictionary for saving model performance results.\n",
    "        best_params (dict): A dictionary to store the best hyperparameters found during optimization.\n",
    "    '''\n",
    "\n",
    "    study = optuna.create_study() \n",
    "    study.optimize(lambda trial: optuna_objective(trial, \n",
    "                                                model_name = 'catboost', \n",
    "                                                optuna_params = cat_optuna_params,\n",
    "                                                X_train = X_train,\n",
    "                                                y_train = y_train, \n",
    "                                                folds = folds, \n",
    "                                                test_metric = test_metric,\n",
    "                                                ), \n",
    "                    n_trials=n_optimization_trials,)\n",
    "    \n",
    "    print('Best params:')\n",
    "    print(study.best_params)\n",
    "    print(f'Training {test_metric} for best params:')\n",
    "    print(study.best_value)\n",
    "\n",
    "    # Visualize optimization run\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "\n",
    "    # Redo cross-validation with best params to get the optimal n_estimators\n",
    "    cv_data = catboost.Pool(\n",
    "            data=X_train,\n",
    "            label=y_train,\n",
    "        )\n",
    "    \n",
    "    cat_params={}\n",
    "    cat_params['learning_rate'] = study.best_params['learning_rate']\n",
    "    cat_params['depth'] = study.best_params['depth']\n",
    "    cat_params['objective'] = study.best_params['objective']\n",
    "    cat_params['eval_metric'] = test_metric\n",
    "\n",
    "    cv_df = catboost.cv(\n",
    "            pool=cv_data,\n",
    "            params=cat_params,\n",
    "            folds=folds,\n",
    "            num_boost_round=1000, \n",
    "            early_stopping_rounds=50, \n",
    "            #verbose_eval=False,\n",
    "            logging_level='Silent',\n",
    "            shuffle=False,\n",
    "        ) \n",
    "    nr_iterations = cv_df.sort_values(f'test-{test_metric}-mean')['iterations'].iloc[0] + 1\n",
    "\n",
    "    # train best catboost model with whole training data\n",
    "    cat_model = catboost.CatBoostRegressor(iterations=nr_iterations, verbose=False, **cat_params)\n",
    "    cat_model.fit(X_train, y_train)\n",
    "\n",
    "    # test predictions\n",
    "    y_predicted = pd.Series(\n",
    "        cat_model.predict(X_test),\n",
    "        index=X_test.index\n",
    "    )\n",
    "\n",
    "    print('CATBOOST')\n",
    "    metrics[\"test\"] = calculate_performance_metrics(y_test, y_predicted)\n",
    "    best_params[\"catboost\"] = study.best_params\n",
    "    best_params[\"catboost\"]['nr_estimators'] = nr_iterations\n",
    "    \n",
    "    explainer = shap.Explainer(cat_model)\n",
    "    shap_values = explainer(X_train.sample(10000, random_state=42) if len(X_train)>10000 else X_train);\n",
    "    shap.plots.bar(shap_values, max_display=10)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    return metrics, best_params, cat_model, shap_values, study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "filepath = Path('../../data/modelling/fpl_df.csv')\n",
    "df = pd.read_csv(filepath, index_col=0)\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_no_shift = ['element_type']\n",
    "\n",
    "features_shift = ['corners_and_indirect_freekicks_order', 'creativity_rank', \n",
    "       'direct_freekicks_order', 'ict_index_rank', 'influence_rank',\n",
    "       'minutes', 'now_cost', 'penalties_order', 'points_per_game', \n",
    "       'selected_by_percent', 'threat_rank',\n",
    "       'team_xG_ewm_5', 'team_xG_ewm_10', 'team_xG_ewm_20',\n",
    "       'team_xG_ewm_40', 'team_xGA_ewm_5', 'team_xGA_ewm_10',\n",
    "       'team_xGA_ewm_20', 'team_xGA_ewm_40', \n",
    "       'opponent_xG_ewm_5', 'opponent_xG_ewm_10',\n",
    "       'opponent_xG_ewm_20', 'opponent_xG_ewm_40', 'opponent_xGA_ewm_5',\n",
    "       'opponent_xGA_ewm_10', 'opponent_xGA_ewm_20',\n",
    "       'opponent_xGA_ewm_40', \n",
    "       'gameweek_assists_ewm_5', 'gameweek_bps_ewm_5',\n",
    "       'gameweek_creativity_ewm_5', 'event_points_ewm_5',\n",
    "       'gameweek_goals_scored_ewm_5', 'gameweek_goals_conceded_ewm_5',\n",
    "       'gameweek_saves_ewm_5', 'gameweek_threat_ewm_5',\n",
    "       'gameweek_xG_ewm_5', 'gameweek_xA_ewm_5', 'gameweek_xGA_ewm_5',\n",
    "       'gameweek_minutes_ewm_5', 'gameweek_xPoints_ewm_5',\n",
    "       'gameweek_assists_ewm_10', 'gameweek_bps_ewm_10',\n",
    "       'gameweek_creativity_ewm_10', 'event_points_ewm_10',\n",
    "       'gameweek_goals_scored_ewm_10', 'gameweek_goals_conceded_ewm_10',\n",
    "       'gameweek_saves_ewm_10', 'gameweek_threat_ewm_10',\n",
    "       'gameweek_xG_ewm_10', 'gameweek_xA_ewm_10', 'gameweek_xGA_ewm_10',\n",
    "       'gameweek_minutes_ewm_10', 'gameweek_xPoints_ewm_10',\n",
    "       'gameweek_assists_ewm_20', 'gameweek_bps_ewm_20',\n",
    "       'gameweek_creativity_ewm_20', 'event_points_ewm_20',\n",
    "       'gameweek_goals_scored_ewm_20', 'gameweek_goals_conceded_ewm_20',\n",
    "       'gameweek_saves_ewm_20', 'gameweek_threat_ewm_20',\n",
    "       'gameweek_xG_ewm_20', 'gameweek_xA_ewm_20', 'gameweek_xGA_ewm_20',\n",
    "       'gameweek_minutes_ewm_20', 'gameweek_xPoints_ewm_20',\n",
    "       'gameweek_assists_ewm_40', 'gameweek_bps_ewm_40',\n",
    "       'gameweek_creativity_ewm_40', 'event_points_ewm_40',\n",
    "       'gameweek_goals_scored_ewm_40', 'gameweek_goals_conceded_ewm_40',\n",
    "       'gameweek_saves_ewm_40', 'gameweek_threat_ewm_40',\n",
    "       'gameweek_xG_ewm_40', 'gameweek_xA_ewm_40', 'gameweek_xGA_ewm_40',\n",
    "       'gameweek_minutes_ewm_40', 'gameweek_xPoints_ewm_40',\n",
    "       'gameweek_assists_expanding', 'gameweek_bps_expanding',\n",
    "       'gameweek_creativity_expanding', 'event_points_expanding',\n",
    "       'gameweek_goals_scored_expanding',\n",
    "       'gameweek_goals_conceded_expanding', 'gameweek_saves_expanding',\n",
    "       'gameweek_threat_expanding', 'gameweek_xG_expanding',\n",
    "       'gameweek_xA_expanding', 'gameweek_xGA_expanding',\n",
    "       'gameweek_minutes_expanding', 'gameweek_xPoints_expanding',\n",
    "       'gameweek_assists_expanding_per90', 'gameweek_bps_expanding_per90',\n",
    "       'gameweek_creativity_expanding_per90',\n",
    "       'event_points_expanding_per90',\n",
    "       'gameweek_goals_scored_expanding_per90',\n",
    "       'gameweek_goals_conceded_expanding_per90',\n",
    "       'gameweek_saves_expanding_per90',\n",
    "       'gameweek_threat_expanding_per90', 'gameweek_xG_expanding_per90',\n",
    "       'gameweek_xA_expanding_per90', 'gameweek_xGA_expanding_per90',\n",
    "       'gameweek_xPoints_expanding_per90', 'xG_overperformance'\n",
    "    ]\n",
    "\n",
    "target = ['event_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift give features\n",
    "df[features_shift] = df.groupby('web_name')[features_shift].shift(shift_param)\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.web_name=='Kane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isnull().sum(axis=1) > 4).sum() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where too much data missing\n",
    "df = df[df.isnull().sum(axis=1) <= 4].reset_index(drop=True)\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_no_shift + features_shift].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "display(X.shape)\n",
    "display(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of season 22-23 data relative to all data\n",
    "df[df.season=='22-23'].shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use season 22-23 for testing, rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ix = df[df.season!='22-23'].index\n",
    "test_ix = df[df.season=='22-23'].index\n",
    "print(f'Train data size: {len(train_ix)}')\n",
    "print(f'Test data size: {len(test_ix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_ix)\n",
    "display(test_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.loc[train_ix]\n",
    "X_test = X.loc[test_ix]\n",
    "y_train = y.loc[train_ix]\n",
    "y_test = y.loc[test_ix]\n",
    "\n",
    "display(X_train)\n",
    "display(X_test)\n",
    "display(y_train)\n",
    "display(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = calculate_performance_metrics(y_train, X_train['points_per_game'])\n",
    "test_metrics = calculate_performance_metrics(y_test, X_test['points_per_game'])\n",
    "\n",
    "results = pd.DataFrame((train_metrics, test_metrics), index=('train', 'test'), columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = calculate_performance_metrics(y_train, X_train['gameweek_xPoints_ewm_5'])\n",
    "test_metrics = calculate_performance_metrics(y_test, X_test['gameweek_xPoints_ewm_5'])\n",
    "\n",
    "results = pd.DataFrame((train_metrics, test_metrics), index=('train', 'test'), columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = calculate_performance_metrics(y_train, X_train['gameweek_xPoints_ewm_10'])\n",
    "test_metrics = calculate_performance_metrics(y_test, X_test['gameweek_xPoints_ewm_10'])\n",
    "\n",
    "results = pd.DataFrame((train_metrics, test_metrics), index=('train', 'test'), columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = calculate_performance_metrics(y_train, X_train['gameweek_xPoints_ewm_20'])\n",
    "test_metrics = calculate_performance_metrics(y_test, X_test['gameweek_xPoints_ewm_20'])\n",
    "\n",
    "results = pd.DataFrame((train_metrics, test_metrics), index=('train', 'test'), columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = calculate_performance_metrics(y_train, X_train['gameweek_xPoints_ewm_40'])\n",
    "test_metrics = calculate_performance_metrics(y_test, X_test['gameweek_xPoints_ewm_40'])\n",
    "\n",
    "results = pd.DataFrame((train_metrics, test_metrics), index=('train', 'test'), columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "                ('scaler', StandardScaler()),               \n",
    "                ('mean_imputer', SimpleImputer(strategy='mean',)),\n",
    "                ('regression', linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13), \n",
    "                                                cv=KFold(n_splits=4, shuffle=False))),\n",
    "        ])\n",
    "\n",
    "model.fit(X_train, y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "test_metrics = calculate_performance_metrics(y_test, y_pred)\n",
    "results = pd.DataFrame(np.array([test_metrics]), index=['test'], columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DEFAULT PARAMS\n",
    "\n",
    "cat_model = catboost.CatBoostRegressor()\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "# test predictions\n",
    "y_predicted = pd.Series(\n",
    "    cat_model.predict(X_test),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "test_metrics = calculate_performance_metrics(y_test, y_predicted)\n",
    "results = pd.DataFrame(np.array([test_metrics]), index=['test'], columns=('MAE', 'RMSE', 'r2'))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "cat_optuna_params = {}\n",
    "cat_optuna_params['cat_learning_rate_bounds'] = [0.001, 0.5]\n",
    "cat_optuna_params['cat_depth_bounds'] = [3,10]\n",
    "cat_optuna_params['cat_objective_list'] = ['RMSE','MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION\n",
    "n_trials = 2\n",
    "\n",
    "folds = KFold(n_splits=4, shuffle=False) \n",
    "test_metric='RMSE'\n",
    "\n",
    "best_params = {}\n",
    "metrics = {}\n",
    "metrics, best_params, cat_model, shap_values, study = cat_optuna(X_train, y_train, X_test, y_test, \n",
    "               cat_optuna_params, best_params, metrics, folds, test_metric, n_optimization_trials=n_trials)\n",
    "\n",
    "print('OPTIMIZATION TRIALS DATA')\n",
    "test_scores = [study.get_trials()[i].values[0] for i in range(0,len(study.get_trials()))]\n",
    "params = [pd.DataFrame(study.get_trials()[i].params, index=[i]) for i in range(0,len(study.get_trials()))]\n",
    "trial_data = pd.concat(params)\n",
    "trial_data[f'test {test_metric}'] = test_scores\n",
    "display(trial_data)\n",
    "\n",
    "print('BEST PARAMETERS TEST SCORES')\n",
    "results = pd.DataFrame(np.array([metrics['test']]), index=['test'], columns=('MAE', 'RMSE', 'r2'))\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = [study.get_trials()[i].values[0] for i in range(0,len(study.get_trials()))]\n",
    "params = [pd.DataFrame(study.get_trials()[i].params, index=[i]) for i in range(0,len(study.get_trials()))]\n",
    "trial_data = pd.concat(params)\n",
    "trial_data[f'test {test_metric}'] = test_scores\n",
    "display(trial_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.get_trials()[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv23-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
